BASE:
  gpu_id: '0'
  algorithm: Clas
  dataset_name: DogCat
  resume: False
  resume_file: ''
  seed: 999
  image_shape: [3, 224, 224]
  n_epoch: 200
  init_lr: 0.001
  start_val: 0
  show_step: 20
  checkpoints: ./checkpoints
  save_epoch: 10
  pretrained_model: ""
  topk: 5
  use_mix: False
  use_apex: False
  use_tensorboard: True


BACKBONES:
  type: LeNet5
  in_channel: 3
  num_classes: 2

LR_SCHEDULER:
  type: 'StepLR'
  params:
    step_size: 30
    gamma: 0.1

OPTIMIZER:
  type: 'SGDDecay'
  params:
    momentum: 0.9
    weight_decay: 0.0001

LOSS:
  type: 'CrossEntropyLoss'


TRAIN:
  batch_size: 32
  num_workers: 4
  shuffle: True
  dataset:
    type: 'ReaderDogCat'
    ann_file: "/zzf/data/split_catdog/train.txt"
    classes: ['dog','cat']
    pipeline:
      - type: Resize
        params:
          size: [32,32]
      - type: ToTensor
      - type: Normalize
        params:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]

VALID:
  batch_size: 32
  num_workers: 4
  shuffle: False
  dataset:
    type: 'ReaderDogCat'
    ann_file: "/zzf/data/split_catdog/valid.txt"
    classes: ['dog','cat']
    pipeline:
      - type: Resize
        params:
          size: [32,32]
      - type: ToTensor
      - type: Normalize
        params:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]